#!/bin/bash

#PBS -N HadoopJob
#PBS -q workq
#PBS -l select=4:ncpus=16
#PBS -l place=scatter
#PBS -l walltime=2:00:00
#PBS -j oe

### 0. Setup environment variables
cd $PBS_O_WORKDIR
export HADOOP_HOME=/home/pxuan/software/hadoop-1.2.1
export MH_HOME=/home/pxuan/software/myHadoop
export PATH=$HADOOP_HOME/bin:$MH_HOME/bin:$PATH
export HADOOP_CONF_DIR=$PBS_O_WORKDIR/hadoop-conf
export PBS_NUM_NODES=`cat $PBS_NODEFILE | uniq | wc -l`
export JAVA_HOME=/home/pxuan/software/jdk-1.7

### 1. Setup Hadoop configuration files
myhadoop-configure.sh -s /local_scratch/$USER/$PBS_JOBID

### 2. Start Hadoop services 
$HADOOP_HOME/bin/start-all.sh
sleep 50

### 3. Copy input files to HDFS (stage-in)
$HADOOP_HOME/bin/hadoop dfs -mkdir data
$HADOOP_HOME/bin/hadoop dfs -put $HADOOP_HOME/CHANGES.txt data/

### 4. Run MapReduce job
$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/hadoop-examples-*.jar wordcount data wordcount-output

### 5. Copy output files from HDFS to your home directory (stage-out)
$HADOOP_HOME/bin/hadoop dfs -get wordcount-output ./

### 6. Stop Hadoop services
$HADOOP_HOME/bin/stop-all.sh

### 7. Clearup
myhadoop-cleanup.sh
